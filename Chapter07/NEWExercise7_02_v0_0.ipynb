{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qualitatively Evaluate a pre-trained DistilBERT model on a couple of sentence pairs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "import Levenshtein\n",
    "import tensorflow as tf\n",
    "import transformers\n",
    "from transformers import TFDistilBertForSequenceClassification\n",
    "from transformers import DistilBertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_word(word,avail,hash_dict):\n",
    "    try:\n",
    "        return (hash_dict[word],avail,hash_dict)\n",
    "    except:\n",
    "        character=np.random.choice(avail,size=1)[0]\n",
    "        hash_dict[word]=character\n",
    "        avail.remove(character)\n",
    "        return (hash_dict[word],avail,hash_dict)\n",
    "    \n",
    "#We represent each word in the pair of sentences by a unique randomly selected character\n",
    "def hash_sentence(sentence1,sentence2):\n",
    "    hash_dict={}\n",
    "    avail=[character for character in string.printable]\n",
    "    st1=''\n",
    "    for word in sentence1.split():\n",
    "        st1_add,avail,hash_dict=hash_word(word, avail,hash_dict)\n",
    "        st1=st1+st1_add\n",
    "    st2=''\n",
    "    for word in sentence2.split():\n",
    "        st2_add,avail,hash_dict=hash_word(word, avail,hash_dict)\n",
    "        st2=st2+st2_add\n",
    "    return (st1,st2)\n",
    "\n",
    "#We convert string to an ordered string containing its unique characters only\n",
    "def bow(st):\n",
    "    x=list(set(st))\n",
    "    x.sort()\n",
    "    y=''\n",
    "    for i in x:\n",
    "        y=y+i\n",
    "    return (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_proc_check(sentence1,sentence2):\n",
    "    st1,st2=hash_sentence(sentence1,sentence2)\n",
    "    l1=len(st1)\n",
    "    l2=len(st2)\n",
    "    l_word_dist=Levenshtein.distance(sentence1,sentence2)\n",
    "    l_bow_word_dist=Levenshtein.distance(bow(st1),bow(st2))\n",
    "    if len(set.intersection(set(st1),set(st2)))>=3:\n",
    "        print('Minimum number of common words okay')\n",
    "    else:\n",
    "        print('Minimum number of common words NOT okay')\n",
    "    if (l1>=1) and (l1<=50) and (l2>=1) and (l2<=50):\n",
    "        print('Length okay')\n",
    "    else:\n",
    "        print('Length NOT okay')\n",
    "    if (l1/l2<=1.5) or (l2/l1<=1.5):\n",
    "        print('Length ratio okay')\n",
    "    else:\n",
    "        print('Length ratio NOT okay')\n",
    "    if (l_word_dist>=1) and (l_word_dist<=20):\n",
    "        print('Word-based Levenshtein edit distance okay')\n",
    "    else:\n",
    "        print('Word-based Levenshtein edit distance NOT okay')\n",
    "    if l_bow_word_dist>=8:\n",
    "        print('Bag of words lexical distance okay')\n",
    "    else:\n",
    "        print('Bag of words lexical distance NOT okay')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TFDistilBertForSequenceClassification.from_pretrained('./data/paraphrase/')\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum number of common words NOT okay\n",
      "Length okay\n",
      "Length ratio okay\n",
      "Word-based Levenshtein edit distance NOT okay\n",
      "Bag of words lexical distance okay\n"
     ]
    }
   ],
   "source": [
    "s1x = \"One never forgets cycling.\"\n",
    "s1y=\"Cycling is a skill that stays with you forever.\"\n",
    "pre_proc_check(s1x,s1y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability that the second sentence is a paraphrase of the first sentence: 0.16632225\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH=max(len(s1x),len(s1y))\n",
    "inputs1_x_y= tokenizer.encode_plus(s1x, s1y,add_special_tokens=True,truncation_strategy='do_not_truncate',max_length=MAX_LENGTH, pad_to_max_length=True,return_tensors='tf')\n",
    "print('The probability that the second sentence is a paraphrase of the first sentence:',tf.nn.softmax(model(inputs1_x_y['input_ids'])[0][0])[1].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum number of common words NOT okay\n",
      "Length okay\n",
      "Length ratio okay\n",
      "Word-based Levenshtein edit distance NOT okay\n",
      "Bag of words lexical distance okay\n"
     ]
    }
   ],
   "source": [
    "s2x = \"Comics are a great source of enjoyment for the reader.\"\n",
    "s2y = \"It is great fun to read comics.\"\n",
    "pre_proc_check(s2x,s2y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability that the second sentence is a paraphrase of the first sentence: 0.582867\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH=max(len(s2x),len(s2y))\n",
    "inputs2_x_y= tokenizer.encode_plus(s2x, s2y,add_special_tokens=True,truncation_strategy='do_not_truncate',max_length=MAX_LENGTH, pad_to_max_length=True,return_tensors='tf')\n",
    "print('The probability that the second sentence is a paraphrase of the first sentence:',tf.nn.softmax(model(inputs2_x_y['input_ids'])[0][0])[1].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
