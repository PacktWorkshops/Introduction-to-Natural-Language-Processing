{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Top Keywords from the news article\n",
    "In this notebook, we will perform the activity of extracting top keywords from news article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk import download, stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The below statement will download the stop word list\n",
    "# 'nltk_data/corpora/stopwords/' at home directory of your computer\n",
    "download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(file_path):\n",
    "    # load the new article\n",
    "    news = ''.join([line for line in open(file_path)])\n",
    "    return news\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def to_lower_case(text):\n",
    "    \"\"\"\n",
    "    >>> to_lower_case(\"This Is a Test String\")\n",
    "    'this is a test string'\n",
    "    \"\"\"\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wht = WhitespaceTokenizer()\n",
    "def tokenize_text(text):\n",
    "    return wht.tokenize(text=text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "def remove_stop_words(token_list):\n",
    "    \"\"\"\n",
    "    remove_stop_words([\"this\", \"is\", \"a\", \"test\", \"string\"])\n",
    "    ['test', 'string']\n",
    "    \"\"\"\n",
    "    return [word for word in token_list if word not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = stem.PorterStemmer()\n",
    "def get_stems(token_list):\n",
    "    \"\"\"\n",
    "    >>> get_stems([\"this\", \"string\", \"is\", \"for\", \"testing\"])\n",
    "    ['thi', 'string', 'is', 'for', 'test']\n",
    "    \"\"\"\n",
    "    return [stemmer.stem(word) for word in token_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_freq(stems):\n",
    "    \"\"\"\n",
    "    >>> get_freq([\"this\", \"is\", \"a\",\"string\",\"this\",\"is\",\"for\",\"testing\"])['is']\n",
    "    2\n",
    "    \"\"\"\n",
    "    freq_dict = {}\n",
    "    for t in stems:\n",
    "        freq_dict[t.strip()] = freq_dict.get(t.strip(), 0) + 1\n",
    "    return freq_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_words(freq_dict, n):\n",
    "    sorted_dict = sorted(freq_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return [x[0] for x in sorted_dict][:n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../data/news_article.txt\"\n",
    "news_article = load_file(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_case_news_art = to_lower_case(text=news_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenize_text(lower_case_news_art)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_tokens = remove_stop_words(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stems = get_stems(removed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dict = get_freq(stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_keywords = get_top_n_words(freq_dict, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import doctest\n",
    "doctest.testmod(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
